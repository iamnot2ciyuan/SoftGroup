"""
FOR-instanceæ•°æ®é›†å¤„ç†ï¼Œç®€åŒ–ç‰ˆ
ä»é¢„å¤„ç†åçš„.pthæ–‡ä»¶åŠ è½½æ•°æ®
"""
import os.path as osp
from glob import glob
from pathlib import Path

import numpy as np
import torch

from .custom import CustomDataset


class FORInstanceDataset(CustomDataset):

    CLASSES = ('low_vegetation', 'terrain', 'tree')
    NYU_ID = None

    def __init__(self,
                 data_root,
                 prefix,
                 suffix,
                 voxel_cfg=None,
                 training=True,
                 with_label=True,
                 repeat=1,
                 logger=None):
        self.data_root = data_root
        self.prefix = prefix
        self.suffix = suffix
        self.voxel_cfg = voxel_cfg
        self.training = training
        self.with_label = with_label
        self.repeat = repeat
        self.logger = logger
        self.mode = 'train' if training else 'test'
        self.filenames = self.get_filenames()
        self.logger.info(f'Load {self.mode} dataset: {len(self.filenames)} scans')

    def get_filenames(self):
        """è·å–æ–‡ä»¶åˆ—è¡¨ï¼Œæ”¯æŒæŒ‰splitåˆ’åˆ†"""
        # è¯»å–æ•°æ®åˆ†å‰²ä¿¡æ¯
        split_file = osp.join(self.data_root, 'data_split_metadata.csv')
        if osp.exists(split_file):
            import pandas as pd
            df = pd.read_csv(split_file)
            
            # æ ¹æ®prefixç­›é€‰
            if self.prefix == 'train':
                # devæ•°æ®çš„80%ä½œä¸ºè®­ç»ƒé›†
                dev_files = df[df['split'] == 'dev']
                split_files = dev_files.iloc[len(dev_files)//5:]
            elif self.prefix == 'val':
                # devæ•°æ®çš„20%ä½œä¸ºéªŒè¯é›†
                dev_files = df[df['split'] == 'dev']
                split_files = dev_files.iloc[:len(dev_files)//5]
            elif self.prefix == 'test':
                split_files = df[df['split'] == 'test']
            else:
                split_files = df
            
            # æ„å»ºå®Œæ•´è·¯å¾„
            filenames = []
            for _, row in split_files.iterrows():
                las_path = osp.join(self.data_root, row['path'])
                if osp.exists(las_path):
                    filenames.append(las_path)
        else:
            # å¦‚æœæ²¡æœ‰åˆ†å‰²æ–‡ä»¶ï¼Œç›´æ¥æœç´¢
            pattern = osp.join(self.data_root, self.prefix, '*' + self.suffix)
            filenames = glob(pattern)
        
        assert len(filenames) > 0, f'Empty dataset for {self.prefix}'
        filenames = sorted(filenames * self.repeat)
        return filenames

    def load(self, filename):
        """
        åŠ è½½é¢„å¤„ç†åçš„.pthæ–‡ä»¶
        å¦‚æœä¼ å…¥çš„æ˜¯.lasè·¯å¾„ï¼Œè‡ªåŠ¨é‡å®šå‘åˆ°é¢„å¤„ç†åçš„.pth
        è¿”å›: xyz, rgb, semantic_label, instance_label
        """
        # å¦‚æœä¼ å…¥çš„æ˜¯ .las è·¯å¾„ï¼Œè‡ªåŠ¨é‡å®šå‘åˆ°é¢„å¤„ç†åçš„ .pth
        if filename.endswith('.las'):
            scan_name = osp.basename(filename).replace('.las', '.pth')
            # æ ¹æ® prefix ç¡®å®š split æ–‡ä»¶å¤¹
            split_folder = self.prefix  # prefix åº”è¯¥æ˜¯ 'train' æˆ– 'val' æˆ– 'test'
            # æ–°çš„è·¯å¾„: dataset/forinstance/preprocess/train/xxx.pth
            filename = osp.join(self.data_root, 'preprocess', split_folder, scan_name)
        
        # æé€ŸåŠ è½½
        xyz, rgb, semantic_label, instance_label = torch.load(filename)
        return xyz, rgb, semantic_label, instance_label

    def transform_train(self, xyz, rgb, semantic_label, instance_label, aug_prob=1.0):
        """
        è®­ç»ƒæ—¶çš„æ•°æ®å˜æ¢ï¼Œç®€åŒ–ç‰ˆ
        å› ä¸º xyz å·²ç»åœ¨é¢„å¤„ç†æ—¶å½’ä¸€åŒ–äº† (xyz -= min)ï¼Œè¿™é‡Œé€»è¾‘éå¸¸ç®€å•
        
        ğŸš¨ å…³é”®ï¼šxyz_middle å¿…é¡»å§‹ç»ˆä¸ xyz ä¿æŒåŒæ­¥ï¼Œä¸”éƒ½æ˜¯ä½“ç´ å•ä½
        - xyz_middle ç”¨äºè®¡ç®— Offset GT (pt_offset_label = pt_mean - xyz_middle)
        - å¦‚æœ xyz_middle å•ä½ä¸ä¸€è‡´ï¼ˆä¸€ä¼šå„¿ç±³ï¼Œä¸€ä¼šå„¿ä½“ç´ ï¼‰ï¼ŒOffset Loss ä¼šå‰§çƒˆæ³¢åŠ¨ï¼ˆ0.3 vs 14.7ï¼‰
        - å› æ­¤ï¼Œxyz_middle å¿…é¡»å§‹ç»ˆæ˜¯ä½“ç´ å•ä½ï¼Œä¸ xyz å®Œå…¨ä¸€è‡´
        
        ğŸš¨ğŸš¨ğŸš¨ æœ€ç»ˆä¿®å¤ï¼šä½“ç´ ç¼©æ”¾å¿…é¡»æ˜¯ç¬¬ä¸€ä¸ªä¸»è¦æ“ä½œ ğŸš¨ğŸš¨ğŸš¨
        """
        # ğŸš¨ğŸš¨ğŸš¨ ä¿®æ­£ï¼šå°†ä½“ç´ ç¼©æ”¾æ“ä½œç§»åŠ¨åˆ°æœ€å‰é¢ ğŸš¨ğŸš¨ğŸš¨
        # 1. ç¼©æ”¾ (Scale) - xyz å˜ä¸ºä½“ç´ å•ä½ï¼ˆå¿…é¡»æ˜¯ç¬¬ä¸€ä¸ªæ“ä½œï¼ï¼‰
        xyz = xyz * self.voxel_cfg.scale
        
        # ğŸš¨ å…³é”®ï¼šxyz_middle åœ¨ä½“ç´ å•ä½ä¸‹ï¼Œç”¨äºè®¡ç®— Offset GT
        # ä»è¿™ä¸€æ­¥å¼€å§‹ï¼Œxyz_middle å¿…é¡»å§‹ç»ˆä¸ xyz ä¿æŒåŒæ­¥
        xyz_middle = xyz.copy()
        
        # 2. æ•°æ®å¢å¼º (Jitter, Flip, Rotate) - åœ¨ä½“ç´ ç©ºé—´ä¸­è¿›è¡Œ
        xyz = self.dataAugment(xyz, True, True, True, False, aug_prob)
        # åŒæ­¥æ›´æ–° xyz_middle
        xyz_middle = xyz.copy()
        
        # 3. Elastic (åœ¨ä½“ç´ ç©ºé—´ä¸­è¿›è¡Œ)
        if np.random.rand() < aug_prob:
            xyz = self.elastic(xyz, 6, 40.)
            xyz = self.elastic(xyz, 20, 160.)
            # ğŸš¨ å…³é”®ï¼šåœ¨ Elastic ååŒæ­¥æ›´æ–° xyz_middle
            xyz_middle = xyz.copy()
        
        # 4. å°†åæ ‡åŸç‚¹ç§»åˆ°æœ€å°å€¼
        xyz = xyz - xyz.min(0)
        xyz_middle = xyz_middle - xyz_middle.min(0)
        
        # 5. Crop (æ”¹è¿›ç‰ˆï¼šåŸºäºä¸­å¿ƒçš„å›ºå®šçª—å£è£å‰ª)
        max_tries = 10  # å¢åŠ é‡è¯•æ¬¡æ•°ï¼Œå› ä¸ºæ–°ç­–ç•¥åŸºäºç‚¹ä¸­å¿ƒï¼ŒæˆåŠŸç‡æ›´é«˜
        valid_idxs = None
        xyz_offset = None
        
        for _ in range(max_tries):
            # è°ƒç”¨æ–°çš„ crop æ–¹æ³•
            xyz_offset, valid_idxs = self.crop(xyz)
            
            if valid_idxs.sum() >= self.voxel_cfg.min_npoint:
                break
        
        if valid_idxs is None or valid_idxs.sum() < self.voxel_cfg.min_npoint:
            # å¦‚æœç‚¹æ•°å¤ªå°‘ï¼Œè¿”å›Noneè®©DataLoaderè·³è¿‡
            return None
        
        # åº”ç”¨ Crop
        # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨ crop è¿”å›çš„ xyz_offset (å·²ç»å¹³ç§»åˆ°äº†å±€éƒ¨åæ ‡ 0,0,0)
        xyz = xyz_offset[valid_idxs]
        
        # ğŸš¨ å…³é”®ï¼šxyz_middle ä¹Ÿå¿…é¡»åº”ç”¨åŒæ ·çš„å¹³ç§»å’Œè¿‡æ»¤ï¼
        # æ–°çš„ crop æ–¹æ³•è¿”å›çš„ xyz_offset = xyz - min_boundï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å¯¹ xyz_middle åšåŒæ ·çš„å¹³ç§»
        # ç”±äº crop å†…éƒ¨æ˜¯éšæœºè®¡ç®—çš„ offsetï¼Œæˆ‘ä»¬éœ€è¦æŠŠ offset ä¼ å‡ºæ¥ï¼Œæˆ–è€…
        # ç®€å•ç‚¹ï¼šç›´æ¥è®© xyz_middle = xyz (å› ä¸ºå®ƒä»¬åœ¨ Scale åæ˜¯å®Œå…¨ä¸€æ ·çš„)
        # ä½†ä¸ºäº†ä¿æŒä¸€è‡´æ€§ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®— offset å¹¶åº”ç”¨åˆ° xyz_middle
        # ç”±äºæ–°çš„ crop æ–¹æ³•å†…éƒ¨è®¡ç®—äº† min_boundï¼Œæˆ‘ä»¬éœ€è¦é‡æ–°è®¡ç®— offset
        # å®é™…ä¸Šï¼Œç”±äº xyz_middle å’Œ xyz åœ¨ Scale åå®Œå…¨ä¸€è‡´ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨ xyz_offset
        # ä½†ä¸ºäº†å®‰å…¨ï¼Œæˆ‘ä»¬é‡æ–°è®¡ç®—ï¼šæ‰¾åˆ°è£å‰ªæ¡†çš„ min_bound
        # ç”±äº crop æ–¹æ³•è¿”å›çš„ xyz_offset = xyz - min_boundï¼Œæ‰€ä»¥ min_bound = xyz - xyz_offset
        # ä½†è¿™æ ·è®¡ç®—ä¼šæœ‰ç²¾åº¦é—®é¢˜ï¼Œæ›´ç®€å•çš„æ–¹æ³•æ˜¯ï¼šç”±äº xyz_middle å’Œ xyz åœ¨ Scale åå®Œå…¨ä¸€è‡´
        # æˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨ç›¸åŒçš„ valid_idxs å’Œç›¸åŒçš„åç§»é€»è¾‘
        # æœ€å®‰å…¨çš„æ–¹æ³•ï¼šç›´æ¥è®© xyz_middle = xyzï¼ˆå› ä¸ºå®ƒä»¬åœ¨æ‰€æœ‰å˜æ¢ä¸­éƒ½ä¿æŒåŒæ­¥ï¼‰
        xyz_middle = xyz.copy()
        
        rgb = rgb[valid_idxs]
        semantic_label = semantic_label[valid_idxs]
        instance_label = self.getCroppedInstLabel(instance_label, valid_idxs)
        
        return xyz, xyz_middle, rgb, semantic_label, instance_label

    def getCroppedInstLabel(self, instance_label, valid_idxs):
        """
        é‡æ–°æ˜ å°„å®ä¾‹æ ‡ç­¾ï¼Œä¿æŒ class_id * 1000 + instance_id æ ¼å¼
        """
        instance_label = instance_label[valid_idxs]
        ins_label_map = {}
        new_id = 0
        instance_ids = np.unique(instance_label)
        for id in instance_ids:
            if id == -100:
                ins_label_map[id] = id
                continue
            # æå–class_idå’Œinstance_id
            class_id = id // 1000
            # é‡æ–°æ˜ å°„instance_idï¼Œä½†ä¿æŒclass_idä¸å˜
            ins_label_map[id] = class_id * 1000 + new_id
            new_id += 1
        instance_label = np.vectorize(ins_label_map.__getitem__)(instance_label)
        return instance_label

    def getInstanceInfo(self, xyz, instance_label, semantic_label):
        """
        è·å–å®ä¾‹ä¿¡æ¯
        ğŸš¨ [æ ¸å¿ƒä¿®æ”¹] å®ç°0.5ç±³æ ‘å¹²è´¨å¿ƒçº¦æŸ
        """
        # æ³¨æ„ï¼šinstance_labelç°åœ¨æ˜¯ class_id * 1000 + instance_id æ ¼å¼
        # getInstanceInfoæœŸæœ›è¿ç»­çš„å®ä¾‹IDï¼ˆ0,1,2,...ï¼‰ï¼Œæ‰€ä»¥éœ€è¦å…ˆè½¬æ¢
        instance_label_continuous = instance_label.copy()
        unique_inst_ids = np.unique(instance_label)
        unique_inst_ids = unique_inst_ids[unique_inst_ids != -100]
        
        # å°† class_id * 1000 + instance_id æ˜ å°„å›è¿ç»­IDç”¨äºgetInstanceInfo
        inst_id_map = {}
        for idx, inst_id in enumerate(unique_inst_ids):
            inst_id_map[inst_id] = idx
        
        for inst_id, new_id in inst_id_map.items():
            instance_label_continuous[instance_label == inst_id] = new_id
        
        # ğŸš¨ğŸš¨ğŸš¨ [æ ¸å¿ƒä¿®æ”¹] å®ç°0.5ç±³æ ‘å¹²è´¨å¿ƒçº¦æŸ ğŸš¨ğŸš¨ğŸš¨
        # æ³¨æ„ï¼šxyz æ˜¯ xyz_middleï¼ˆä½“ç´ å•ä½ï¼‰ï¼Œéœ€è¦è½¬æ¢ä¸ºç±³å•ä½æ¥è®¡ç®—0.5ç±³çº¦æŸ
        scale = self.voxel_cfg.scale if self.voxel_cfg else 10.0
        xyz_meters = xyz / scale  # è½¬æ¢ä¸ºç±³å•ä½
        
        pt_mean = np.ones((xyz.shape[0], 3), dtype=np.float32) * -100.0
        instance_pointnum = []
        instance_cls = []
        instance_num = max(int(instance_label_continuous.max()) + 1, 0)
        
        for i_ in range(instance_num):
            inst_idx_i = np.where(instance_label_continuous == i_)
            if inst_idx_i[0].size == 0:
                continue
                
            xyz_i_meters = xyz_meters[inst_idx_i]  # ç±³å•ä½
            
            # ğŸš¨ æå–åº•éƒ¨0.5ç±³çš„ç‚¹ï¼ˆæ ‘å¹²éƒ¨åˆ†ï¼‰
            # 1. æ‰¾åˆ°æœ€å°Zå€¼ï¼ˆæŠ—å™ªï¼šä½¿ç”¨ç¬¬3å°çš„Zå€¼ä½œä¸ºåŸºå‡†ï¼‰
            if len(xyz_i_meters) > 10:
                k = min(3, len(xyz_i_meters) - 1)
                min_z = np.partition(xyz_i_meters[:, 2], k)[k]
            else:
                min_z = xyz_i_meters[:, 2].min()
            
            # 2. æˆªå–åº•éƒ¨0.5ç±³èŒƒå›´çš„ç‚¹
            base_mask = xyz_i_meters[:, 2] <= (min_z + 0.5)
            base_points = xyz_i_meters[base_mask]
            
            if len(base_points) > 0:
                # 3. è®¡ç®—æ ‘å¹²è´¨å¿ƒï¼ˆç±³å•ä½ï¼‰
                stem_center_meters = np.mean(base_points, axis=0)
                # è½¬æ¢ä¸ºä½“ç´ å•ä½
                stem_center = stem_center_meters * scale
                pt_mean[inst_idx_i] = stem_center
            else:
                # å¦‚æœæ‰¾ä¸åˆ°æ ‘åŸºï¼ˆæ¯”å¦‚æ ‘è¢«åˆ‡æ–­åªå‰©æ ‘å† ï¼‰ï¼Œä½¿ç”¨æ•´æ£µæ ‘çš„è´¨å¿ƒä½œä¸ºfallback
                pt_mean[inst_idx_i] = xyz_i_meters.mean(0) * scale
            
            instance_pointnum.append(inst_idx_i[0].size)
            cls_idx = inst_idx_i[0][0]
            instance_cls.append(semantic_label[cls_idx])
        
        # è®¡ç®— offset labelï¼ˆä½“ç´ å•ä½ï¼‰
        # pt_mean å’Œ xyz éƒ½æ˜¯ä½“ç´ å•ä½ï¼Œæ‰€ä»¥ pt_offset_label ä¹Ÿæ˜¯ä½“ç´ å•ä½
        pt_offset_label = pt_mean - xyz
        
        # ğŸš¨ ä¿®å¤3: instance_clsåº”è¯¥æ˜¯å®ä¾‹ç±»åˆ«ç¼–å·ï¼ˆ0-basedï¼‰ï¼Œä¸æ˜¯è¯­ä¹‰ç±»åˆ«ç¼–å·
        # é…ç½®ä¸­instance_classes=3ï¼Œä½†å®é™…åªæœ‰æ ‘éœ€è¦å®ä¾‹åˆ†å‰²
        # è¯­ä¹‰ç±»åˆ«2ï¼ˆtreeï¼‰åº”è¯¥æ˜ å°„åˆ°å®ä¾‹ç±»åˆ«2ï¼ˆå› ä¸ºè¯­ä¹‰æ ‡ç­¾æ˜¯0-basedï¼š0=low_veg, 1=terrain, 2=treeï¼‰
        # instance_cls å·²ç»æ˜¯ä» semantic_label è·å–çš„ï¼Œæ‰€ä»¥æ ‘(è¯­ä¹‰2) -> å®ä¾‹ç±»åˆ«2
        # ä½†æ ¹æ®å®é™…éœ€æ±‚ï¼Œåªæœ‰æ ‘éœ€è¦å®ä¾‹åˆ†å‰²ï¼Œæ‰€ä»¥å°†éæ ‘ç±»åˆ«è®¾ä¸º-100
        # æ³¨æ„ï¼šå¦‚æœé…ç½®ä¸­instance_classes=3ï¼Œåˆ™ä¿æŒåŸæ ·ï¼›å¦‚æœinstance_classes=1ï¼Œåˆ™æ˜ å°„ä¸º0
        # è¿™é‡Œæ ¹æ®é…ç½®ä¿æŒåŸæ ·ï¼Œå› ä¸ºinstance_classes=3
        
        # éªŒè¯ï¼šç¡®ä¿æ‰€æœ‰æœ‰æ•ˆçš„å®ä¾‹éƒ½æ˜¯æ ‘ç±»åˆ«ï¼ˆè¯­ä¹‰2ï¼‰
        # å¦‚æœå‘ç°éæ ‘ç±»åˆ«çš„å®ä¾‹ï¼Œè®°å½•è­¦å‘Šï¼ˆä½†ä¸å½±å“è®­ç»ƒï¼‰
        if len(instance_cls) > 0:
            valid_instances = [i for i, cls in enumerate(instance_cls) if cls != -100]
            if len(valid_instances) > 0:
                # æ£€æŸ¥å¯¹åº”çš„è¯­ä¹‰æ ‡ç­¾æ˜¯å¦éƒ½æ˜¯2ï¼ˆæ ‘ï¼‰
                for inst_idx in valid_instances:
                    # æ‰¾åˆ°è¯¥å®ä¾‹å¯¹åº”çš„ç‚¹
                    inst_mask = (instance_label_continuous == inst_idx)
                    if inst_mask.any():
                        inst_sem_labels = np.unique(semantic_label[inst_mask])
                        # å¦‚æœå®ä¾‹ä¸­æœ‰é2çš„è¯­ä¹‰æ ‡ç­¾ï¼Œè®°å½•è­¦å‘Šï¼ˆè¯­ä¹‰æ ‡ç­¾æ˜¯0-basedï¼Œæ ‘æ˜¯ç±»åˆ«2ï¼‰
                        if len(inst_sem_labels) > 1 or (len(inst_sem_labels) == 1 and inst_sem_labels[0] != 2):
                            import logging
                            logger = logging.getLogger()
                            logger.warning(f"å®ä¾‹ {inst_idx} åŒ…å«éæ ‘ç±»åˆ«çš„è¯­ä¹‰æ ‡ç­¾: {inst_sem_labels}")
        
        return instance_num, instance_pointnum, instance_cls, pt_offset_label

