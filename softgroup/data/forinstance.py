"""
FOR-instanceæ•°æ®é›†å¤„ç†ï¼Œç®€åŒ–ç‰ˆ
ä»é¢„å¤„ç†åçš„.pthæ–‡ä»¶åŠ è½½æ•°æ®
"""
import os.path as osp
from glob import glob
from pathlib import Path

import numpy as np
import torch

from .custom import CustomDataset


class FORInstanceDataset(CustomDataset):

    CLASSES = ('low_vegetation', 'terrain', 'tree')
    NYU_ID = None

    def __init__(self,
                 data_root,
                 prefix,
                 suffix,
                 voxel_cfg=None,
                 training=True,
                 with_label=True,
                 repeat=1,
                 logger=None):
        self.data_root = data_root
        self.prefix = prefix
        self.suffix = suffix
        self.voxel_cfg = voxel_cfg
        self.training = training
        self.with_label = with_label
        self.repeat = repeat
        self.logger = logger
        self.mode = 'train' if training else 'test'
        self.filenames = self.get_filenames()
        self.logger.info(f'Load {self.mode} dataset: {len(self.filenames)} scans')

    def get_filenames(self):
        """è·å–æ–‡ä»¶åˆ—è¡¨ï¼Œæ”¯æŒæŒ‰splitåˆ’åˆ†"""
        # è¯»å–æ•°æ®åˆ†å‰²ä¿¡æ¯
        split_file = osp.join(self.data_root, 'data_split_metadata.csv')
        if osp.exists(split_file):
            import pandas as pd
            df = pd.read_csv(split_file)
            
            # æ ¹æ®prefixç­›é€‰
            if self.prefix == 'train':
                # devæ•°æ®çš„80%ä½œä¸ºè®­ç»ƒé›†
                dev_files = df[df['split'] == 'dev']
                split_files = dev_files.iloc[len(dev_files)//5:]
            elif self.prefix == 'val':
                # devæ•°æ®çš„20%ä½œä¸ºéªŒè¯é›†
                dev_files = df[df['split'] == 'dev']
                split_files = dev_files.iloc[:len(dev_files)//5]
            elif self.prefix == 'test':
                split_files = df[df['split'] == 'test']
            else:
                split_files = df
            
            # æ„å»ºå®Œæ•´è·¯å¾„
            filenames = []
            for _, row in split_files.iterrows():
                las_path = osp.join(self.data_root, row['path'])
                if osp.exists(las_path):
                    filenames.append(las_path)
        else:
            # å¦‚æœæ²¡æœ‰åˆ†å‰²æ–‡ä»¶ï¼Œç›´æ¥æœç´¢
            pattern = osp.join(self.data_root, self.prefix, '*' + self.suffix)
            filenames = glob(pattern)
        
        assert len(filenames) > 0, f'Empty dataset for {self.prefix}'
        filenames = sorted(filenames * self.repeat)
        return filenames

    def load(self, filename):
        """
        åŠ è½½é¢„å¤„ç†åçš„.pthæ–‡ä»¶
        å¦‚æœä¼ å…¥çš„æ˜¯.lasè·¯å¾„ï¼Œè‡ªåŠ¨é‡å®šå‘åˆ°é¢„å¤„ç†åçš„.pth
        è¿”å›: xyz, rgb, semantic_label, instance_label
        """
        # å¦‚æœä¼ å…¥çš„æ˜¯ .las è·¯å¾„ï¼Œè‡ªåŠ¨é‡å®šå‘åˆ°é¢„å¤„ç†åçš„ .pth
        if filename.endswith('.las'):
            scan_name = osp.basename(filename).replace('.las', '.pth')
            # æ ¹æ® prefix ç¡®å®š split æ–‡ä»¶å¤¹
            split_folder = self.prefix  # prefix åº”è¯¥æ˜¯ 'train' æˆ– 'val' æˆ– 'test'
            # æ–°çš„è·¯å¾„: dataset/forinstance/preprocess/train/xxx.pth
            filename = osp.join(self.data_root, 'preprocess', split_folder, scan_name)
        
        # æé€ŸåŠ è½½
        xyz, rgb, semantic_label, instance_label = torch.load(filename)
        return xyz, rgb, semantic_label, instance_label

    def transform_train(self, xyz, rgb, semantic_label, instance_label, aug_prob=1.0):
        """
        è®­ç»ƒæ—¶çš„æ•°æ®å˜æ¢ï¼Œç®€åŒ–ç‰ˆ
        å› ä¸º xyz å·²ç»åœ¨é¢„å¤„ç†æ—¶å½’ä¸€åŒ–äº† (xyz -= min)ï¼Œè¿™é‡Œé€»è¾‘éå¸¸ç®€å•
        
        ğŸš¨ å…³é”®ï¼šxyz_middle å¿…é¡»å§‹ç»ˆä¸ xyz ä¿æŒåŒæ­¥ï¼Œä¸”éƒ½æ˜¯ä½“ç´ å•ä½
        - xyz_middle ç”¨äºè®¡ç®— Offset GT (pt_offset_label = pt_mean - xyz_middle)
        - å¦‚æœ xyz_middle å•ä½ä¸ä¸€è‡´ï¼ˆä¸€ä¼šå„¿ç±³ï¼Œä¸€ä¼šå„¿ä½“ç´ ï¼‰ï¼ŒOffset Loss ä¼šå‰§çƒˆæ³¢åŠ¨ï¼ˆ0.3 vs 14.7ï¼‰
        - å› æ­¤ï¼Œxyz_middle å¿…é¡»å§‹ç»ˆæ˜¯ä½“ç´ å•ä½ï¼Œä¸ xyz å®Œå…¨ä¸€è‡´
        
        ğŸš¨ğŸš¨ğŸš¨ æœ€ç»ˆä¿®å¤ï¼šä½“ç´ ç¼©æ”¾å¿…é¡»æ˜¯ç¬¬ä¸€ä¸ªä¸»è¦æ“ä½œ ğŸš¨ğŸš¨ğŸš¨
        """
        # ğŸš¨ğŸš¨ğŸš¨ ä¿®æ­£ï¼šå°†ä½“ç´ ç¼©æ”¾æ“ä½œç§»åŠ¨åˆ°æœ€å‰é¢ ğŸš¨ğŸš¨ğŸš¨
        # 1. ç¼©æ”¾ (Scale) - xyz å˜ä¸ºä½“ç´ å•ä½ï¼ˆå¿…é¡»æ˜¯ç¬¬ä¸€ä¸ªæ“ä½œï¼ï¼‰
        xyz = xyz * self.voxel_cfg.scale
        
        # ğŸš¨ å…³é”®ï¼šxyz_middle åœ¨ä½“ç´ å•ä½ä¸‹ï¼Œç”¨äºè®¡ç®— Offset GT
        # ä»è¿™ä¸€æ­¥å¼€å§‹ï¼Œxyz_middle å¿…é¡»å§‹ç»ˆä¸ xyz ä¿æŒåŒæ­¥
        xyz_middle = xyz.copy()
        
        # 2. æ•°æ®å¢å¼º (Jitter, Flip, Rotate) - åœ¨ä½“ç´ ç©ºé—´ä¸­è¿›è¡Œ
        xyz = self.dataAugment(xyz, True, True, True, False, aug_prob)
        # åŒæ­¥æ›´æ–° xyz_middle
        xyz_middle = xyz.copy()
        
        # 3. Elastic (åœ¨ä½“ç´ ç©ºé—´ä¸­è¿›è¡Œ)
        if np.random.rand() < aug_prob:
            xyz = self.elastic(xyz, 6, 40.)
            xyz = self.elastic(xyz, 20, 160.)
            # ğŸš¨ å…³é”®ï¼šåœ¨ Elastic ååŒæ­¥æ›´æ–° xyz_middle
            xyz_middle = xyz.copy()
        
        # 4. å°†åæ ‡åŸç‚¹ç§»åˆ°æœ€å°å€¼
        xyz = xyz - xyz.min(0)
        xyz_middle = xyz_middle - xyz_middle.min(0)
        
        # 5. Crop (æ”¹è¿›ç‰ˆï¼šåŸºäºä¸­å¿ƒçš„å›ºå®šçª—å£è£å‰ª)
        max_tries = 10  # å¢åŠ é‡è¯•æ¬¡æ•°ï¼Œå› ä¸ºæ–°ç­–ç•¥åŸºäºç‚¹ä¸­å¿ƒï¼ŒæˆåŠŸç‡æ›´é«˜
        valid_idxs = None
        xyz_offset = None
        
        for _ in range(max_tries):
            # è°ƒç”¨æ–°çš„ crop æ–¹æ³•
            xyz_offset, valid_idxs = self.crop(xyz)
            
            if valid_idxs.sum() >= self.voxel_cfg.min_npoint:
                break
        
        if valid_idxs is None or valid_idxs.sum() < self.voxel_cfg.min_npoint:
            # å¦‚æœç‚¹æ•°å¤ªå°‘ï¼Œè¿”å›Noneè®©DataLoaderè·³è¿‡
            return None
        
        # åº”ç”¨ Crop
        # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨ crop è¿”å›çš„ xyz_offset (å·²ç»å¹³ç§»åˆ°äº†å±€éƒ¨åæ ‡ 0,0,0)
        xyz = xyz_offset[valid_idxs]
        
        # ğŸš¨ å…³é”®ï¼šxyz_middle ä¹Ÿå¿…é¡»åº”ç”¨åŒæ ·çš„å¹³ç§»å’Œè¿‡æ»¤ï¼
        # æ–°çš„ crop æ–¹æ³•è¿”å›çš„ xyz_offset = xyz - min_boundï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å¯¹ xyz_middle åšåŒæ ·çš„å¹³ç§»
        # ç”±äº crop å†…éƒ¨æ˜¯éšæœºè®¡ç®—çš„ offsetï¼Œæˆ‘ä»¬éœ€è¦æŠŠ offset ä¼ å‡ºæ¥ï¼Œæˆ–è€…
        # ç®€å•ç‚¹ï¼šç›´æ¥è®© xyz_middle = xyz (å› ä¸ºå®ƒä»¬åœ¨ Scale åæ˜¯å®Œå…¨ä¸€æ ·çš„)
        # ä½†ä¸ºäº†ä¿æŒä¸€è‡´æ€§ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®— offset å¹¶åº”ç”¨åˆ° xyz_middle
        # ç”±äºæ–°çš„ crop æ–¹æ³•å†…éƒ¨è®¡ç®—äº† min_boundï¼Œæˆ‘ä»¬éœ€è¦é‡æ–°è®¡ç®— offset
        # å®é™…ä¸Šï¼Œç”±äº xyz_middle å’Œ xyz åœ¨ Scale åå®Œå…¨ä¸€è‡´ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨ xyz_offset
        # ä½†ä¸ºäº†å®‰å…¨ï¼Œæˆ‘ä»¬é‡æ–°è®¡ç®—ï¼šæ‰¾åˆ°è£å‰ªæ¡†çš„ min_bound
        # ç”±äº crop æ–¹æ³•è¿”å›çš„ xyz_offset = xyz - min_boundï¼Œæ‰€ä»¥ min_bound = xyz - xyz_offset
        # ä½†è¿™æ ·è®¡ç®—ä¼šæœ‰ç²¾åº¦é—®é¢˜ï¼Œæ›´ç®€å•çš„æ–¹æ³•æ˜¯ï¼šç”±äº xyz_middle å’Œ xyz åœ¨ Scale åå®Œå…¨ä¸€è‡´
        # æˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨ç›¸åŒçš„ valid_idxs å’Œç›¸åŒçš„åç§»é€»è¾‘
        # æœ€å®‰å…¨çš„æ–¹æ³•ï¼šç›´æ¥è®© xyz_middle = xyzï¼ˆå› ä¸ºå®ƒä»¬åœ¨æ‰€æœ‰å˜æ¢ä¸­éƒ½ä¿æŒåŒæ­¥ï¼‰
        xyz_middle = xyz.copy()
        
        rgb = rgb[valid_idxs]
        semantic_label = semantic_label[valid_idxs]
        instance_label = self.getCroppedInstLabel(instance_label, valid_idxs)
        
        return xyz, xyz_middle, rgb, semantic_label, instance_label

    def getCroppedInstLabel(self, instance_label, valid_idxs):
        """
        é‡æ–°æ˜ å°„å®ä¾‹æ ‡ç­¾ï¼Œä¿æŒ class_id * 1000 + instance_id æ ¼å¼
        """
        instance_label = instance_label[valid_idxs]
        ins_label_map = {}
        new_id = 0
        instance_ids = np.unique(instance_label)
        for id in instance_ids:
            if id == -100:
                ins_label_map[id] = id
                continue
            # æå–class_idå’Œinstance_id
            class_id = id // 1000
            # é‡æ–°æ˜ å°„instance_idï¼Œä½†ä¿æŒclass_idä¸å˜
            ins_label_map[id] = class_id * 1000 + new_id
            new_id += 1
        instance_label = np.vectorize(ins_label_map.__getitem__)(instance_label)
        return instance_label

    def getInstanceInfo(self, xyz, instance_label, semantic_label):
        """
        è·å–å®ä¾‹ä¿¡æ¯
        ğŸš¨ ä¿®å¤3: ç¡®ä¿è¯­ä¹‰ç±»åˆ«æ˜ å°„æ­£ç¡®
        """
        # æ³¨æ„ï¼šinstance_labelç°åœ¨æ˜¯ class_id * 1000 + instance_id æ ¼å¼
        # getInstanceInfoæœŸæœ›è¿ç»­çš„å®ä¾‹IDï¼ˆ0,1,2,...ï¼‰ï¼Œæ‰€ä»¥éœ€è¦å…ˆè½¬æ¢
        instance_label_continuous = instance_label.copy()
        unique_inst_ids = np.unique(instance_label)
        unique_inst_ids = unique_inst_ids[unique_inst_ids != -100]
        
        # å°† class_id * 1000 + instance_id æ˜ å°„å›è¿ç»­IDç”¨äºgetInstanceInfo
        inst_id_map = {}
        for idx, inst_id in enumerate(unique_inst_ids):
            inst_id_map[inst_id] = idx
        
        for inst_id, new_id in inst_id_map.items():
            instance_label_continuous[instance_label == inst_id] = new_id
        
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•
        # æ³¨æ„ï¼šä¼ å…¥çš„ xyz åº”è¯¥æ˜¯ xyz_middleï¼ˆä½“ç´ å•ä½ï¼‰
        # çˆ¶ç±»ä¼šè®¡ç®— pt_offset_label = pt_mean - xyzï¼Œæ‰€ä»¥ pt_offset_label ä¹Ÿåº”è¯¥æ˜¯ä½“ç´ å•ä½
        ret = super().getInstanceInfo(xyz, instance_label_continuous, semantic_label)
        instance_num, instance_pointnum, instance_cls, pt_offset_label = ret
        
        # ğŸš¨ğŸš¨ğŸš¨ ç»ˆæå®‰å…¨å«å£«ï¼šå¼ºåˆ¶ä¿®æ­£ pt_offset_label çš„å•ä½ ğŸš¨ğŸš¨ğŸš¨
        # å¦‚æœ Offset Loss ä»ç„¶çˆ†ç‚¸ï¼ˆ> 10ï¼‰ï¼Œè¯´æ˜ xyz_middle åœ¨æŸäº›æ‰¹æ¬¡ä¸­ä»ç„¶æ˜¯ç±³å•ä½
        # è¿™é‡Œå¼ºåˆ¶å°† pt_offset_label é™¤ä»¥ scaleï¼Œç¡®ä¿å®ƒå§‹ç»ˆæ˜¯ä½“ç´ å•ä½
        # è¿™æ˜¯ä¸€ä¸ª"åå‘ä¿®æ­£"æ–¹æ¡ˆï¼Œå³ä½¿ xyz_middle æ˜¯ç±³å•ä½ï¼Œä¹Ÿèƒ½å¾—åˆ°æ­£ç¡®çš„ä½“ç´ å•ä½ offset
        
        # ğŸš¨ ç¡®ä¿ scale å­˜åœ¨ä¸”æœ‰æ•ˆ
        if self.voxel_cfg is None or not hasattr(self.voxel_cfg, 'scale'):
            import logging
            logger = logging.getLogger()
            logger.error("voxel_cfg.scale ä¸å­˜åœ¨ï¼æ— æ³•ä¿®æ­£ pt_offset_label å•ä½ï¼")
        else:
            scale = self.voxel_cfg.scale
            # ğŸš¨ å¼ºåˆ¶ä¿®æ­£ï¼šå°† pt_offset_label é™¤ä»¥ scale
            # å¦‚æœ xyz_middle æ˜¯ç±³å•ä½ï¼Œpt_offset_label ä¹Ÿæ˜¯ç±³å•ä½ï¼Œé™¤ä»¥ scale å¾—åˆ°ä½“ç´ å•ä½
            # å¦‚æœ xyz_middle å·²ç»æ˜¯ä½“ç´ å•ä½ï¼Œè¿™é‡Œé™¤ä»¥ scale ä¼šå¾—åˆ°é”™è¯¯çš„å•ä½ï¼ˆç±³å•ä½ï¼‰
            # ä½†æ ¹æ® Offset Loss çˆ†ç‚¸çš„ç°è±¡ï¼Œè¯´æ˜åœ¨æŸäº›æƒ…å†µä¸‹ xyz_middle ä»ç„¶æ˜¯ç±³å•ä½
            pt_offset_label = pt_offset_label / scale
            
            # ğŸš¨ è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥ä¿®æ­£åçš„æ•°å€¼èŒƒå›´
            if isinstance(pt_offset_label, np.ndarray) and pt_offset_label.size > 0:
                max_offset = np.abs(pt_offset_label).max()
                if max_offset > 10.0:
                    import logging
                    logger = logging.getLogger()
                    logger.warning(f"pt_offset_label ä¿®æ­£åä»ç„¶å¾ˆå¤§ (max={max_offset:.2f})ï¼Œå¯èƒ½ä»æœ‰å•ä½é—®é¢˜ï¼")
        
        # ğŸš¨ ä¿®å¤3: instance_clsåº”è¯¥æ˜¯å®ä¾‹ç±»åˆ«ç¼–å·ï¼ˆ0-basedï¼‰ï¼Œä¸æ˜¯è¯­ä¹‰ç±»åˆ«ç¼–å·
        # é…ç½®ä¸­instance_classes=1ï¼Œåªæœ‰æ ‘ç±»åˆ«éœ€è¦å®ä¾‹åˆ†å‰²
        # è¯­ä¹‰ç±»åˆ«2ï¼ˆtreeï¼‰åº”è¯¥æ˜ å°„åˆ°å®ä¾‹ç±»åˆ«0ï¼ˆå› ä¸ºè¯­ä¹‰æ ‡ç­¾æ˜¯0-basedï¼š0=low_veg, 1=terrain, 2=treeï¼‰
        # æ³¨æ„ï¼šåªæœ‰è¯­ä¹‰ç±»åˆ«2ï¼ˆæ ‘ï¼‰æ‰éœ€è¦å®ä¾‹åˆ†å‰²ï¼Œå…¶ä»–ç±»åˆ«ï¼ˆ0,1ï¼‰ä¸åº”è¯¥æœ‰å®ä¾‹
        # å¦‚æœinstance_clsä¸­æœ‰é2çš„å€¼ï¼Œè¯´æ˜æ•°æ®æœ‰é—®é¢˜ï¼Œåº”è¯¥è®¾ä¸º-100ï¼ˆå¿½ç•¥ï¼‰
        instance_cls = [0 if x == 2 else -100 for x in instance_cls]  # æ ‘(è¯­ä¹‰2) -> å®ä¾‹ç±»åˆ«0
        
        # éªŒè¯ï¼šç¡®ä¿æ‰€æœ‰æœ‰æ•ˆçš„å®ä¾‹éƒ½æ˜¯æ ‘ç±»åˆ«ï¼ˆè¯­ä¹‰3ï¼‰
        # å¦‚æœå‘ç°éæ ‘ç±»åˆ«çš„å®ä¾‹ï¼Œè®°å½•è­¦å‘Šï¼ˆä½†ä¸å½±å“è®­ç»ƒï¼‰
        if len(instance_cls) > 0:
            valid_instances = [i for i, cls in enumerate(instance_cls) if cls != -100]
            if len(valid_instances) > 0:
                # æ£€æŸ¥å¯¹åº”çš„è¯­ä¹‰æ ‡ç­¾æ˜¯å¦éƒ½æ˜¯3
                for inst_idx in valid_instances:
                    # æ‰¾åˆ°è¯¥å®ä¾‹å¯¹åº”çš„ç‚¹
                    inst_mask = (instance_label_continuous == inst_idx)
                    if inst_mask.any():
                        inst_sem_labels = np.unique(semantic_label[inst_mask])
                        # å¦‚æœå®ä¾‹ä¸­æœ‰é2çš„è¯­ä¹‰æ ‡ç­¾ï¼Œè®°å½•è­¦å‘Šï¼ˆè¯­ä¹‰æ ‡ç­¾æ˜¯0-basedï¼Œæ ‘æ˜¯ç±»åˆ«2ï¼‰
                        if len(inst_sem_labels) > 1 or (len(inst_sem_labels) == 1 and inst_sem_labels[0] != 2):
                            import logging
                            logger = logging.getLogger()
                            logger.warning(f"å®ä¾‹ {inst_idx} åŒ…å«éæ ‘ç±»åˆ«çš„è¯­ä¹‰æ ‡ç­¾: {inst_sem_labels}")
        
        # ğŸš¨ è¿”å›æ—¶ç¡®ä¿ pt_offset_label æ²¡æœ‰ä¹˜ scale
        return instance_num, instance_pointnum, instance_cls, pt_offset_label

