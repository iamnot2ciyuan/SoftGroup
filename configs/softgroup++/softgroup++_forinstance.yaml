model:
  channels: 32
  num_blocks: 6        # [关键修复] 从 7 改为 6，减少一层下采样，避免BatchNorm错误
  semantic_classes: 3   # 0=low_vegetation, 1=terrain, 2=tree
  instance_classes: 1   # 全部树归为1类实例
  sem2ins_classes: [2]  # 对语义类别2（tree）做实例分割
  semantic_only: False
  semantic_weight: [1.0, 1.0, 1.0]
  ignore_label: -100
  with_coords: True

  grouping_cfg:
    with_pyramid: False
    pyramid_base_size: 20
    with_octree: True
    # [核心修复] 降低分数组阈值，让更多低置信度的点也能参与聚类
    score_thr: 0.2    # 从0.5降低到0.2，让更多点参与聚类
    # [核心修复] 增大聚类半径：从 12 (1.2m) 增大到 20 (2.0m)
    # 20 体素 * 0.1m/体素 = 2.0m 物理半径
    # 鉴于模型的 Offset 误差仍在 2.7 米左右，2 米的搜索半径有很大机会捕获到飘散的树冠点
    # 这是打破当前"零正样本"僵局的强力手段
    radius: 20          # 20个体素 = 2.0米 (10cm体素 * 20 = 2.0米)
    mean_active: 300   # [终极修复] 验证集OOM的终极杀手锏：从2000降到300（ScanNet默认值）
    class_numpoint_mean: [-1., -1., -1.]
    # [建议] 保持较低的点数阈值，确保即使圈到的点不多也能形成 Proposal
    npoint_thr: 20     # 保持较低值，确保即使圈到的点不多也能形成 Proposal
    ignore_classes: [0, 1]  # 只忽略low_vegetation和terrain，对tree(2)做聚类

  instance_voxel_cfg:
    scale: 10            # 10cm体素，更适合森林
    spatial_shape: 512

  train_cfg:
    lvl_fusion: True
    max_proposal_num: 1000
    # [核心修复] 降低正样本门槛：从 0.5 降低到 0.25
    # 树木形状不规则，且点云稀疏，很难达到 0.5 的重叠度
    # 只要生成的框和 GT 有 25% 的重叠就算正样本，先让 num_pos > 0 再说
    pos_iou_thr: 0.25
    match_low_quality: True
    # [核心修复] 降低最小正样本阈值：从 0.3 降低到 0.25
    min_pos_thr: 0.25

  test_cfg:
    x4_split: False
    cls_score_thr: 0.05
    mask_score_thr: 0.5
    min_npoint: 50
    eval_tasks: ['semantic']  # [最终绕过 OOM] 暂时移除 'instance' 评估，只跑 Semantic 评估
  fixed_modules: []

data:
  train:
    type: 'forinstance'
    data_root: 'dataset/forinstance'
    prefix: 'train'
    suffix: '.las'
    training: True
    repeat: 4            # [修复] 降低Batch Size后，增加Repeat次数保证训练数据量
    voxel_cfg:
      scale: 10            # 10cm 体素，更适合森林
      # [核心修复] 新格式: [Z, XY] -> Z轴 256 (25.6米), XY轴 160 (16米)
      # 这是一个固定的取景框，不会再动态缩小了
      spatial_shape: [256, 160]
      # 允许框内有更多点，防止采样导致的稀疏
      max_npoint: 100000   # [修复] 增大到100k，充分利用显存
      # 只要框到了 1000 个点以上就算有效批次
      min_npoint: 1000     # [核心修复] 从 500 提高到 1000，过滤掉太稀疏的样本
  test:
    type: 'forinstance'
    data_root: 'dataset/forinstance'
    prefix: 'val'
    suffix: '.las'
    training: False
    voxel_cfg:
      scale: 10
      # [核心修复] 新格式: [Z, XY] -> Z轴 128 (12.8米), XY轴 128 (12.8米)
      spatial_shape: [128, 128]  # 固定窗口大小，不再动态缩小
      max_npoint: 30000    # [永久 OOM 修复] 相应降低点数限制
      min_npoint: 1000     # [修复] 降低最小点数，适配新的裁剪策略

dataloader:
  train:
    batch_size: 1        # [修复] 降到最小，避免OOM
    num_workers: 4        # [修复] 提高到4，减少Loss波动
  test:
    batch_size: 1
    num_workers: 1

optimizer:
  type: 'Adam'
  lr: 0.002

loss_weights:
  semantic_loss: 1.0
  offset_loss: 0.1
  cls_loss: 1.0
  mask_loss: 1.0
  iou_score_loss: 1.0

fp16: False
epochs: 108
step_epoch: 20
save_freq: 4
log_interval: 10
